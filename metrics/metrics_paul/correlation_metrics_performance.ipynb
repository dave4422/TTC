{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from metrics import calculate_sample_alignment_distance, calculate_sample_alignment_accuracy, calculate_class_alignment_distance, calculate_class_alignment_consistency, calculate_gaussian_potential_uniformity, calculate_probabilistic_entropy_uniformity, pairwise_distances\n",
    "def gen_all_metrics(embeddings, labels):\n",
    "    n_samples = embeddings.shape[0]\n",
    "    all_embeddings = np.concatenate(embeddings, axis=0)\n",
    "    all_labels = np.concatenate(labels, axis=0)\n",
    "\n",
    "\n",
    "    # all_labels = labels\n",
    "    # all_embeddings = embeddings\n",
    "    #\n",
    "\n",
    "    print(all_embeddings.shape, all_labels.shape)\n",
    "\n",
    "    indices_0 = np.where(all_labels == 0)[0]\n",
    "    indices_1 = np.where(all_labels == 1)[0]\n",
    "\n",
    "    # Simulate similarity matrix\n",
    "    sim = pairwise_distances(\n",
    "        all_embeddings, metric=\"euclidean\") + np.eye(all_embeddings.shape[0])\n",
    "\n",
    "    sad_0, sad_1 = calculate_sample_alignment_distance(\n",
    "        sim, n_samples, labels[0])\n",
    "\n",
    "    saa_0, saa_1 = calculate_sample_alignment_accuracy(\n",
    "        sim, n_samples, labels[0],all_labels)\n",
    "\n",
    "    cad_0, cad_1 = calculate_class_alignment_distance(\n",
    "        sim, all_embeddings, all_labels)\n",
    "\n",
    "    cac_0, cac_1 = calculate_class_alignment_consistency(\n",
    "        sim, all_embeddings, all_labels)\n",
    "\n",
    "    # Gaussian Potential Uniformity\n",
    "    gu_0, gu_1 = calculate_gaussian_potential_uniformity(\n",
    "        all_embeddings, all_labels)\n",
    "\n",
    "    # Probabilistic Entropy Uniformity\n",
    "  \n",
    "\n",
    " \n",
    "\n",
    "    print(f\"{sad_0.mean():.3f} & {sad_1.mean():.3f} &\", f\"{(saa_0 / indices_0.shape[0]):.1f} & {(saa_1 / indices_1.shape[0]):.1f}\",\n",
    "          f\" & {np.array(cad_0).mean():.3f} & {np.array(cad_1).mean():.3f} & \",   f\"{cac_0.mean():.1f} & {cac_1.mean():.1f}\", f\" & {gu_0:.3f} & {gu_1:.3f} &\",\n",
    "          )\n",
    "\n",
    "    # Compile results\n",
    "    results = {\n",
    "        \"sample_alignment_distance_cls0\": sad_0,\n",
    "        \"sample_alignment_distance_cls1\": sad_1,\n",
    "        \"sample_alignment_accuracy_cls0\": saa_0,\n",
    "        \"sample_alignment_accuracy_cls1\": saa_1,\n",
    "        \"class_alignment_distance_cls0\": cad_0,\n",
    "        \"class_alignment_distance_cls1\": cad_1,\n",
    "        \"class_alignment_consistency_cls0\": cac_0,\n",
    "        \"class_alignment_consistency_cls1\": cac_1,\n",
    "        \"gaussian_potential_uniformity_cls0\": gu_0,\n",
    "        \"gaussian_potential_uniformity_cls1\": gu_1,\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['supcon','paco','KCLK3','TSCK3', 'SBC','BCL','SupMin','Prot']\n",
    "res = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'selected_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 56\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labels))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# labels = np.array(labels)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# embeddings = np.array(embeddings)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# embeddings = embeddings.reshape(embeddings.shape[1],-1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#print(f'{class_name}_{model}')\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m res[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m gen_all_metrics(embeddings[\u001b[43mselected_indices\u001b[49m], labels[selected_indices])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'selected_indices' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "class_name = 'infarction'\n",
    "re = False\n",
    "\n",
    "# random indices \n",
    "\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    with open(f'/vol/miltank/users/mildenbd/Projects/icml/embeddings_baselines_card/{class_name}/embeddings_5_last_train__{model}.pkl', 'rb') as file:\n",
    "        embeddings = pickle.load(file)\n",
    "        print(len(embeddings))\n",
    "    with open(f'/vol/miltank/users/mildenbd/Projects/icml/embeddings_baselines_card/{class_name}/labels_5_last_train__{model}.pkl', 'rb') as file:\n",
    "        labels = pickle.load(file)\n",
    "\n",
    "    print(len(labels))\n",
    "    # labels = np.array(labels)\n",
    "    # embeddings = np.array(embeddings)\n",
    "    # embeddings = embeddings.reshape(embeddings.shape[1],-1)\n",
    "    # labels = labels.reshape(-1)\n",
    "    # print(labels.shape)\n",
    "    # print(embeddings.shape)\n",
    "    # positive_indices = np.where(labels == 1)[0]\n",
    "    # num_positives = len(positive_indices)\n",
    "    # print(f\"Number of positive samples (label=1): {num_positives}\")\n",
    "    # negative_indices = np.where(labels == 0)[0]\n",
    "    # num_negatives = len(negative_indices)\n",
    "    # print(f\"Number of negative samples (label=0): {num_negatives}\")\n",
    "    \n",
    "  \n",
    "    # if num_negatives < num_positives:\n",
    "    #     print(\"Not enough negative samples to balance the dataset.\")\n",
    "    #     continue  # Skip to the next model\n",
    "  \n",
    "    # sampled_neg_indices = np.random.choice(\n",
    "    #     negative_indices,\n",
    "    #     size=num_positives,\n",
    "    #     replace=False\n",
    "    # )\n",
    "    # print(f\"Sampled {len(sampled_neg_indices)} negative indices.\")\n",
    "    # selected_indices = np.concatenate((positive_indices, sampled_neg_indices))\n",
    "    \n",
    "   \n",
    "    # np.random.shuffle(selected_indices)\n",
    "    # print(\"Shuffled the selected indices.\")\n",
    "    \n",
    "\n",
    "    # subset_labels = labels[selected_indices].tolist()\n",
    "    # subset_embeddings = embeddings[selected_indices].tolist()\n",
    "\n",
    "    #print(f\"Subset labels: {len(labels[selected_indices])}\")\n",
    "\n",
    "    #print(f'{class_name}_{model}')\n",
    "    res[f'{class_name}_{model}'] = gen_all_metrics(embeddings[selected_indices], labels[selected_indices])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pearsonr\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "performance = {\n",
    "    'supcon':61.9\n",
    "    ,'paco':66.6\n",
    "    ,'KCLK3':75.3\n",
    "    ,'TSCK3':75.7,\n",
    "     'SBC':70\n",
    "     ,'BCL':73.4\n",
    "     ,'SupMin':77.7\n",
    "     ,'Prot':77.9\n",
    "}\n",
    "\n",
    "for model, perf in performance.items():\n",
    "    res[model]['performance'] = perf\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(performance, orient='index').reset_index()\n",
    "df = df.rename(columns={'index': 'model'})\n",
    "\n",
    "\n",
    "metrics = [col for col in df.columns if col not in ['model', 'performance']]\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Determine the number of plots\n",
    "num_metrics = len(metrics)\n",
    "cols = 3  # Number of columns in the grid\n",
    "rows = num_metrics // cols + int(num_metrics % cols > 0)\n",
    "\n",
    "# Initialize the matplotlib figure\n",
    "plt.figure(figsize=(cols * 5, rows * 4))\n",
    "\n",
    "for idx, metric in enumerate(metrics, 1):\n",
    "    plt.subplot(rows, cols, idx)\n",
    "    sns.scatterplot(data=df, x=metric, y='performance', s=100)\n",
    "\n",
    "    # Annotate each point with the model name\n",
    "    for i in range(df.shape[0]):\n",
    "        plt.text(x=df[metric][i]+0.001, y=df['performance'][i]+0.001, s=df['model'][i],\n",
    "                 fontdict=dict(color='black', size=9),\n",
    "                 horizontalalignment='left', verticalalignment='bottom')\n",
    "\n",
    "    plt.title(f'Performance vs {metric}')\n",
    "    plt.xlabel(metric)\n",
    "    plt.ylabel('Performance')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Model Performance vs Various Metrics', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Adding Regression Line and Correlation Coefficient\n",
    "plt.figure(figsize=(cols * 5, rows * 4))\n",
    "\n",
    "for idx, metric in enumerate(metrics, 1):\n",
    "    plt.subplot(rows, cols, idx)\n",
    "    sns.regplot(data=df, x=metric, y='performance', scatter=True, ci=None, line_kws={'color': 'red'})\n",
    "\n",
    "    \n",
    "    corr, _ = pearsonr(df[metric], df['performance'])\n",
    "    plt.text(0.05, 0.95, f'r={corr:.2f}', transform=plt.gca().transAxes,\n",
    "             horizontalalignment='left', verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round,pad=0.2', fc='yellow', alpha=0.3))\n",
    "\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        plt.text(x=df[metric][i]+0.001, y=df['performance'][i]+0.001, s=df['model'][i],\n",
    "                 fontdict=dict(color='black', size=9),\n",
    "                 horizontalalignment='left', verticalalignment='bottom')\n",
    "\n",
    "    plt.title(f'Performance vs {metric}')\n",
    "    plt.xlabel(metric)\n",
    "    plt.ylabel('Performance')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Model Performance vs Various Metrics with Correlation', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Normalizing Metrics\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = df.copy()\n",
    "df_normalized[metrics] = scaler.fit_transform(df[metrics])\n",
    "\n",
    "# Plotting Normalized Metrics\n",
    "plt.figure(figsize=(cols * 5, rows * 4))\n",
    "\n",
    "for idx, metric in enumerate(metrics, 1):\n",
    "    plt.subplot(rows, cols, idx)\n",
    "    sns.scatterplot(data=df_normalized, x=metric, y='performance', s=100)\n",
    "\n",
    "    # Annotate each point with the model name\n",
    "    for i in range(df_normalized.shape[0]):\n",
    "        plt.text(x=df_normalized[metric][i]+0.01, y=df_normalized['performance'][i]+0.01, s=df_normalized['model'][i],\n",
    "                 fontdict=dict(color='black', size=9),\n",
    "                 horizontalalignment='left', verticalalignment='bottom')\n",
    "\n",
    "    plt.title(f'Performance vs {metric} (Normalized)')\n",
    "    plt.xlabel(f'{metric} (Normalized)')\n",
    "    plt.ylabel('Performance')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.suptitle('Model Performance vs Various Metrics (Normalized)', fontsize=16, y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
